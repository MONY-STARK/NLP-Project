{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiKN37BRIgxF",
        "outputId": "736b2b0d-4137-465a-aab3-b74afd7630bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "path = \"/content/drive/MyDrive/NLP Datasets/IMDB Dataset.csv\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn0DQVTxbkEu"
      },
      "source": [
        "# Libaries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se5h4Jxbk054"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import nltk\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFFO1_rubyls",
        "outputId": "63a68e5b-75bc-4833-f9e1-e12c9523ecce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4tvPQ-5bqdF"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lCr6cKZDk09b"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(pd.read_csv(r\"/content/drive/MyDrive/NLP Datasets/IMDB Dataset.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eMlJRHmbdWS"
      },
      "source": [
        "#Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MO-YbZu7zClk"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokenized_documents = []  # List to store tokenized words for each document\n",
        "\n",
        "    stopwords_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "    for corpus in text:\n",
        "        corpus = re.sub(\"[^a-zA-Z]\", \" \", corpus)  # Remove non-alphabetic characters\n",
        "        corpus = re.sub(r\"\\s+\", \" \", corpus)       # Remove extra whitespaces\n",
        "        corpus = corpus.lower()                    # Convert to lowercase\n",
        "        words = corpus.split()                     # Tokenize the document into words\n",
        "\n",
        "        lem_words = []\n",
        "        for word in words:\n",
        "            if word not in stopwords_set:\n",
        "                lemmatized = lemmatizer.lemmatize(word)\n",
        "                lem_words.append(lemmatized)\n",
        "        tokenized_documents.append(lem_words)  # Append the tokenized words for this document to the list\n",
        "\n",
        "    return tokenized_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VD-EvYXSqLg4"
      },
      "outputs": [],
      "source": [
        "tokenized_words = text_preprocessing(data[\"review\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KhzuJQBfRwx",
        "outputId": "06554df2-1d9b-4751-98e7-7d2d81e14d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50000\n"
          ]
        }
      ],
      "source": [
        "tw = tokenized_words\n",
        "print(len(tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV20t0JRecR6",
        "outputId": "93100c04-1ec9-41fe-e353-c41df5564107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhm0c7lQeJP4"
      },
      "outputs": [],
      "source": [
        "labels = [0 if sentiment == \"positive\" else 1 for sentiment in data[\"sentiment\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_CWiMbxgn2c"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCM84J1CeJYv"
      },
      "outputs": [],
      "source": [
        "# 1s\n",
        "# print(len(tokenized_words))\n",
        "# 91005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAmFVBRqagMg"
      },
      "source": [
        "###Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1nEK5QyajY9"
      },
      "source": [
        "Only list (i.e) all words in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYsMfDNxT54r"
      },
      "outputs": [],
      "source": [
        "# ['one', 'reviewer', 'mentioned', 'watching', 'oz', 'episode', 'hooked', 'right', 'exactly', 'happened', 'br', 'br',\n",
        "#  'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'go', 'trust',\n",
        "#  'show', 'faint', 'hearted', 'timid', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic',\n",
        "#  'use', 'word', 'br', 'br', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary',\n",
        "#  'focus', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards',\n",
        "#  'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', 'aryan', 'muslim', 'gangsta', 'latino', 'christian', 'italian',\n",
        "#  'irish', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far', 'away', 'br', 'br',\n",
        "#  'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'dare', 'forget', 'pretty', 'picture', 'painted',\n",
        "#  'mainstream', 'audience', 'forget', 'charm', 'forget', 'romance', 'oz', 'mess', 'around', 'first', 'episode', 'ever',\n",
        "#  'saw', 'struck', 'nasty', 'surreal', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high',\n",
        "#  'level', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guard', 'sold', 'nickel', 'inmate', 'kill', 'order',\n",
        "#  'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmate', 'turned', 'prison', 'bitch', 'due', 'lack', 'street',\n",
        "#  'skill', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', 'thats',\n",
        "#  'get', 'touch', 'darker', 'side', 'wonderful', 'little', 'production', 'br', 'br', 'filming', 'technique', 'unassuming',\n",
        "#  'old', 'time', 'bbc', 'fashion', 'give', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece',\n",
        "#  'br', 'br', 'actor', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'got', 'polari', 'voice', 'pat', 'truly', 'see',\n",
        "#  'seamless', 'editing', 'guided', 'reference', 'williams', 'diary', 'entry', 'well', 'worth', 'watching', 'terrificly',\n",
        "#  'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', 'comedy', 'life', 'br', 'br',\n",
        "#  'realism', 'really', 'come', 'home', 'little', 'thing', 'fantasy', 'guard', 'rather', 'use', 'traditional', 'dream',\n",
        "#  'technique', 'remains', 'solid', 'disappears', 'play', 'knowledge', 'sens', 'particularly', 'scene', 'concerning',\n",
        "#  'orton', 'halliwell', 'set', 'particularly', 'flat', 'halliwell', 'mural', 'decorating', 'every', 'surface', 'terribly',\n",
        "#  'well', 'done']\n",
        "# 260"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u6sL9LPa2wr"
      },
      "source": [
        "set used (i.e) Vocalbulary of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwXuA563UYZm"
      },
      "outputs": [],
      "source": [
        "# ['br', 'technique', 'diary', 'get', 'romance', 'fact', 'regard', 'wonderful', 'crooked', 'given', 'viewing', 'pat',\n",
        "#  'unflinching', 'city', 'comforting', 'entire', 'surface', 'mainstream', 'thing', 'face', 'italian', 'pretty', 'picture',\n",
        "#  'oswald', 'irish', 'shady', 'punch', 'ever', 'glass', 'never', 'class', 'reference', 'drug', 'old', 'pull', 'main',\n",
        "#  'ready', 'aryan', 'front', 'sens', 'written', 'polari', 'muslim', 'comedy', 'may', 'brutality', 'high', 'see', 'mainly',\n",
        "#  'struck', 'agreement', 'scuffle', 'taste', 'sense', 'mannered', 'every', 'many', 'knowledge', 'word', 'scene', 'prison',\n",
        "#  'set', 'happened', 'use', 'comfortable', 'accustomed', 'thats', 'actor', 'voice', 'focus', 'life', 'say', 'reviewer',\n",
        "#  'watched', 'performed', 'give', 'agenda', 'violence', 'hooked', 'security', 'turned', 'saw', 'extremely', 'remains',\n",
        "#  'latino', 'terribly', 'touch', 'dream', 'watching', 'street', 'experience', 'traditional', 'level', 'filming', 'side',\n",
        "#  'around', 'injustice', 'piece', 'mentioned', 'home', 'forget', 'developed', 'time', 'flat', 'right', 'realism', 'seamless',\n",
        "#  'experimental', 'fantasy', 'dodgy', 'one', 'decorating', 'little', 'play', 'episode', 'gangsta', 'faint', 'cell',\n",
        "#  'williams', 'exactly', 'far', 'chosen', 'nickel', 'graphic', 'mural', 'oz', 'master', 'really', 'away', 'order',\n",
        "#  'nasty', 'maximum', 'painted', 'terrificly', 'dealing', 'nickname', 'discomforting', 'em', 'darker', 'would', 'sex',\n",
        "#  'section', 'editing', 'privacy', 'orton', 'bbc', 'masterful', 'appeal', 'trust', 'bitch', 'become', 'truly', 'classic',\n",
        "#  'surreal', 'audience', 'done', 'great', 'disappears', 'solid', 'guided', 'uncomfortable', 'michael', 'production', 'sold',\n",
        "#  'come', 'sometimes', 'concerning', 'entry', 'well', 'hardcore', 'got', 'fashion', 'mess', 'skill', 'particularly',\n",
        "#  'middle', 'dare', 'worth', 'state', 'called', 'christian', 'due', 'charm', 'unassuming', 'go', 'death', 'halliwell',\n",
        "#  'emerald', 'lack', 'penitentary', 'hearted', 'inwards', 'first', 'rather', 'inmate', 'kill', 'show', 'sheen', 'guard',\n",
        "#  'stare', 'timid']\n",
        "# 201"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcqOMllcbC9I"
      },
      "source": [
        "# Vector Conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eH39QHXBM5e8"
      },
      "outputs": [],
      "source": [
        "\n",
        "word2vec = Word2Vec(sentences = tokenized_words, vector_size=10, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-ajh-fadnkdo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mxLksuOkxwd"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GXCxoxJUVvdV"
      },
      "outputs": [],
      "source": [
        "word_vectors = {}\n",
        "for corpus_wrods in tokenized_words:\n",
        "    word_vectors[word] = word2vec.wv[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JgX6sHDbM5uV"
      },
      "outputs": [],
      "source": [
        "# (word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r7U0ZOYaf3Oh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Initialize an empty list to store word vectors for each tokenized sentence\n",
        "X = []\n",
        "\n",
        "# Iterate through each tokenized sentence\n",
        "for sentence in tokenized_words:\n",
        "    # Initialize an empty list to store word vectors for this sentence\n",
        "    sentence_vectors = []\n",
        "    # Iterate through each token in the sentence\n",
        "    for token in sentence:\n",
        "        # Check if the token exists in the vocabulary of the Word2Vec model\n",
        "        if token in word_vectors:\n",
        "            # Retrieve the word vector for this token from the Word2Vec model\n",
        "            word_vector = word_vectors[token]\n",
        "            # Append the word vector to the list of word vectors for this sentence\n",
        "            sentence_vectors.append(word_vector)\n",
        "    # Append the list of word vectors for this sentence to the list of sequences\n",
        "    X.append(sentence_vectors)\n",
        "\n",
        "# Pad the sequences to ensure they all have the same length\n",
        "X_padded = pad_sequences(X, dtype='float32', padding='post')  # You can adjust dtype and padding according to your needs\n",
        "\n",
        "# Convert the padded sequences to a numpy array\n",
        "X_np = np.array(X_padded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "36sYIvkRm-wf"
      },
      "outputs": [],
      "source": [
        "#labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eIb7-efmTJP",
        "outputId": "b2db084f-f0eb-4416-d8fe-866bda3efa46"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((50000, 300, 10), (50000,))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "Y = np.array(labels)\n",
        "X_np.shape, Y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdxvRSzHbV4t"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "fL28RzMHbaiO",
        "outputId": "7a6bd2e2-1d21-4a68-aa9b-b13946876549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 100, 10)           500000    \n",
            "                                                                 \n",
            " spatial_dropout1d_3 (Spati  (None, 100, 10)           0         \n",
            " alDropout1D)                                                    \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 100)               44400     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 544501 (2.08 MB)\n",
            "Trainable params: 544501 (2.08 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"spatial_dropout1d_3\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (64, 100, 10, 10)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(64, 100, 10), dtype=float32)\n      • training=True\n      • mask=None\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6f40a8bf45e7>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_4' (type Sequential).\n    \n    Input 0 of layer \"spatial_dropout1d_3\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (64, 100, 10, 10)\n    \n    Call arguments received by layer 'sequential_4' (type Sequential):\n      • inputs=tf.Tensor(shape=(64, 100, 10), dtype=float32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Define maximum sequence length\n",
        "max_len = 100\n",
        "num_words = 50000\n",
        "embedding_dim = 10\n",
        "\n",
        "\n",
        "# Define your LSTM model architecture\n",
        "model = Sequential()\n",
        "model.add(LSTM(units = 100, input_shape(X.shape[1:]), activation = \"tanh\", return_sequence = True))\n",
        "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Padding sequences\n",
        "# X_pad = pad_sequences(list(word_vectors.values()), maxlen=max_len)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_np, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6AMkGh2dt53"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aKfJn0Qdt8C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMA0Wvxfdt-3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_oqaNH1duAy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuVYOfBJduEj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}